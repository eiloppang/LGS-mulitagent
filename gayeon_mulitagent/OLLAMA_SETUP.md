# Ollama ì„¤ì¹˜ ë° ì‚¬ìš© ê°€ì´ë“œ

ì´ í”„ë¡œì íŠ¸ë¥¼ Ollamaë¡œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. **ë¬´ë£Œ**ë¡œ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

## ğŸ“¦ 1ë‹¨ê³„: Ollama ì„¤ì¹˜

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS
```bash
brew install ollama
```

### Windows
https://ollama.com/download ì—ì„œ ë‹¤ìš´ë¡œë“œ

## ğŸš€ 2ë‹¨ê³„: Ollama ì‹¤í–‰

```bash
# Ollama ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
ollama serve
```

ìƒˆ í„°ë¯¸ë„ì„ ì—´ê³  ê³„ì† ì§„í–‰í•˜ì„¸ìš”.

## ğŸ“¥ 3ë‹¨ê³„: í•„ìš”í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ë©”ì¸ ëª¨ë¸ (LLM) - ì•½ 4.7GB
ollama pull llama3.1

# ì„ë² ë”© ëª¨ë¸ (ë²¡í„° ê²€ìƒ‰ìš©) - ì•½ 274MB
ollama pull nomic-embed-text
```

### ì¶”ì²œ ëª¨ë¸ (ì„ íƒì‚¬í•­)

ë” ë‚˜ì€ ì„±ëŠ¥ì´ í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# í•œêµ­ì–´ì— ê°•í•œ ëª¨ë¸ë“¤
ollama pull qwen2.5:7b          # ì•½ 4.7GB, í•œêµ­ì–´ ìš°ìˆ˜
ollama pull gemma2:9b           # ì•½ 5.5GB, ì„±ëŠ¥ ìš°ìˆ˜
ollama pull mistral             # ì•½ 4.1GB, ë¹ ë¥´ê³  íš¨ìœ¨ì 

# í° ëª¨ë¸ (ë” ì¢‹ì€ í’ˆì§ˆ, ë” ëŠë¦¼)
ollama pull llama3.1:70b        # ì•½ 40GB (GPU í•„ìˆ˜)
```

## ğŸ› ï¸ 4ë‹¨ê³„: í”„ë¡œì íŠ¸ ì„¤ì •

```bash
cd /home/work/gayeon_mulitagent

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
cp .env.example .env
```

`.env` íŒŒì¼ (ê¸°ë³¸ê°’ìœ¼ë¡œë„ ë™ì‘):
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

## â–¶ï¸ 5ë‹¨ê³„: ì‹¤í–‰

```bash
# ëŒ€í™”í˜• ì±—ë´‡ ì‹¤í–‰
python main.py

# ë˜ëŠ” ì˜ˆì œ ì‹¤í–‰
python examples.py
```

## ğŸ¯ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### LLM ëª¨ë¸ (ë‹µë³€ ìƒì„±ìš©)

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | í’ˆì§ˆ | í•œêµ­ì–´ | ì¶”ì²œ |
|------|------|------|------|--------|------|
| `llama3.1` | 4.7GB | â­â­â­ | â­â­â­â­ | â­â­â­ | âœ… ê¸°ë³¸ |
| `qwen2.5:7b` | 4.7GB | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | ğŸ‡°ğŸ‡· í•œêµ­ì–´ ìµœê³  |
| `mistral` | 4.1GB | â­â­â­â­ | â­â­â­ | â­â­ | ğŸš€ ë¹ ë¦„ |
| `gemma2:9b` | 5.5GB | â­â­ | â­â­â­â­â­ | â­â­â­ | ğŸ’ í’ˆì§ˆ ìµœê³  |

### ì„ë² ë”© ëª¨ë¸ (ë²¡í„° ê²€ìƒ‰ìš©)

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | í’ˆì§ˆ | ì¶”ì²œ |
|------|------|------|------|------|
| `nomic-embed-text` | 274MB | â­â­â­â­ | â­â­â­â­ | âœ… ê¸°ë³¸ |
| `mxbai-embed-large` | 669MB | â­â­â­ | â­â­â­â­â­ | ğŸ’ í’ˆì§ˆ ìµœê³  |

## ğŸ”§ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

### ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ë¡œ ë³€ê²½

`.env` íŒŒì¼ ìˆ˜ì •:
```bash
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
```

### ë°©ë²• 2: ì½”ë“œì—ì„œ ì§ì ‘ ë³€ê²½

```python
from agents import MultiAgentOrchestrator

orchestrator = MultiAgentOrchestrator(
    talk_style_dir="./GS_talk_style",
    paper_dir="./GS_paper",
    max_retries=3
)

# ê°œë³„ ì—ì´ì „íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
from agents import KnowledgeAgent, StyleAgent, ValidatorAgent

knowledge = KnowledgeAgent(
    model_name="qwen2.5:7b",          # í•œêµ­ì–´ ê°•í™”
    embedding_model="mxbai-embed-large"  # í’ˆì§ˆ í–¥ìƒ
)

style = StyleAgent(
    model_name="gemma2:9b",           # ë†’ì€ í’ˆì§ˆ
    temperature=0.9                    # ë” ì°½ì˜ì 
)

validator = ValidatorAgent(
    model_name="llama3.1",
    temperature=0.2                    # ì¼ê´€ëœ í‰ê°€
)
```

## ğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. GPU ì‚¬ìš© (NVIDIA)
```bash
# GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤
nvidia-smi  # GPU ìƒíƒœ í™•ì¸
```

### 2. ë©”ëª¨ë¦¬ ì ˆì•½
ì‘ì€ ëª¨ë¸ ì‚¬ìš©:
```bash
ollama pull llama3.1:8b    # ê¸°ë³¸ (4.7GB)
ollama pull mistral        # ë” ì‘ìŒ (4.1GB)
ollama pull phi3           # ë§¤ìš° ì‘ìŒ (2.3GB)
```

### 3. ì†ë„ í–¥ìƒ
```python
# chunk_size ì¤„ì´ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,   # ê¸°ë³¸ 1000
    chunk_overlap=50  # ê¸°ë³¸ 100
)

# ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¤„ì´ê¸°
result = knowledge_agent.process({
    "query": "ì§ˆë¬¸",
    "top_k": 3  # ê¸°ë³¸ 5
})
```

## ğŸ› ë¬¸ì œ í•´ê²°

### Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
```bash
# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
systemctl status ollama

# ì¬ì‹œì‘
systemctl restart ollama

# ìˆ˜ë™ ì‹¤í–‰
ollama serve
```

### ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
```bash
# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
- ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (phi3, mistral)
- chunk_size ì¤„ì´ê¸°
- í•œ ë²ˆì— í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ë§Œ ì´ˆê¸°í™”

### ë„ˆë¬´ ëŠë¦¼
- GPU ë“œë¼ì´ë²„ í™•ì¸
- ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš© (mistral)
- top_k ê°’ ì¤„ì´ê¸°

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### OpenAI API vs Ollama

| í•­ëª© | OpenAI | Ollama |
|------|--------|--------|
| ë¹„ìš© | ğŸ’° ìœ ë£Œ ($$$) | ğŸ†“ ë¬´ë£Œ |
| ì†ë„ | ğŸš€ ë§¤ìš° ë¹ ë¦„ | âš¡ ë¹ ë¦„ (GPU í•„ìš”) |
| í’ˆì§ˆ | ğŸ’ ìµœê³  | â­ ìš°ìˆ˜ |
| í”„ë¼ì´ë²„ì‹œ | â˜ï¸ í´ë¼ìš°ë“œ | ğŸ”’ ë¡œì»¬ |
| ì¸í„°ë„· | ğŸ“¡ í•„ìˆ˜ | ğŸ“´ ë¶ˆí•„ìš” |
| ì„¤ì • | âœ… ê°„ë‹¨ | ğŸ”§ ì•½ê°„ ë³µì¡ |

## ğŸ“ ì¶”ê°€ í•™ìŠµ ìë£Œ

- Ollama ê³µì‹ ë¬¸ì„œ: https://ollama.com/docs
- ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: https://ollama.com/library
- Langchain + Ollama: https://python.langchain.com/docs/integrations/llms/ollama

## âš¡ ë¹ ë¥¸ ì‹œì‘ (ìš”ì•½)

```bash
# 1. Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# 2. ì„œë²„ ì‹œì‘
ollama serve &

# 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1
ollama pull nomic-embed-text

# 4. ì˜ì¡´ì„± ì„¤ì¹˜
cd /home/work/gayeon_mulitagent
pip install -r requirements.txt

# 5. ì‹¤í–‰!
python main.py
```

ë! ğŸ‰
