# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (Ollama)

## ì™„ì „íˆ ì²˜ìŒë¶€í„° ì‹œì‘í•˜ê¸°

### 1ï¸âƒ£ Ollama ì„¤ì¹˜

```bash
# Linux/WSL
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows: https://ollama.com/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
```

### 2ï¸âƒ£ Ollama ì„œë²„ ì‹¤í–‰

**ìƒˆ í„°ë¯¸ë„ì„ ì—´ê³ :**
```bash
ollama serve
```

> ğŸ’¡ ì´ í„°ë¯¸ë„ì€ ê³„ì† ì—´ì–´ë‘ì„¸ìš”! ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.

### 3ï¸âƒ£ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

**ë˜ ë‹¤ë¥¸ ìƒˆ í„°ë¯¸ë„ì„ ì—´ê³ :**
```bash
# LLM ëª¨ë¸ (ë‹µë³€ ìƒì„±ìš©)
ollama pull llama3.1

# ì„ë² ë”© ëª¨ë¸ (ê²€ìƒ‰ìš©)
ollama pull nomic-embed-text
```

ë‹¤ìš´ë¡œë“œ ì‹œê°„:
- `llama3.1`: ì•½ 5-10ë¶„ (4.7GB)
- `nomic-embed-text`: ì•½ 1ë¶„ (274MB)

### 4ï¸âƒ£ ëª¨ë¸ ì„¤ì¹˜ í™•ì¸

```bash
ollama list
```

ë‹¤ìŒê³¼ ê°™ì´ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤:
```
NAME                    ID              SIZE    MODIFIED
llama3.1:latest         42182419e950    4.7 GB  2 minutes ago
nomic-embed-text:latest 0a109f422b47    274 MB  1 minute ago
```

### 5ï¸âƒ£ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
cd /home/work/gayeon_mulitagent
pip install -r requirements.txt
```

ì„¤ì¹˜ë˜ëŠ” ì£¼ìš” íŒ¨í‚¤ì§€:
- `langchain`: LLM í”„ë ˆì„ì›Œí¬
- `langchain-community`: Ollama ì—°ë™
- `chromadb`: ë²¡í„° DB
- `pypdf`: PDF ì½ê¸°

### 6ï¸âƒ£ ì‹¤í–‰!

```bash
python main.py
```

## ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì œ 1: ëŒ€í™”í˜• ì±—ë´‡

```bash
python main.py
```

```
ì§ˆë¬¸> ì´ê´‘ìˆ˜ê°€ ì°½ì”¨ê°œëª…ì„ ì–´ë–»ê²Œ ì •ë‹¹í™”í–ˆë‚˜ìš”?

ğŸ” Step 1: ì§€ì‹ ê²€ìƒ‰ ì¤‘...
   - ì°¸ê³  ìë£Œ: 5ê°œ
   - ì¶œì²˜: ì°½ì”¨ê°œëª….pdf, ì´ê´‘ìˆ˜ì˜ ì¹œì¼ì´ë… ë‹¤ì‹œ ì½ê¸°.pdf

âœï¸  Step 2: ì´ê´‘ìˆ˜ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ ì¤‘...

âœ… Step 3: ìŠ¤íƒ€ì¼ ê²€ì¦ ì¤‘...
   - ê²€ì¦ ì ìˆ˜: 78.5/100
   - ì–´íœ˜: 20.0
   - êµ¬ì¡°: 19.0
   - ì–´ì¡°: 20.5
   - ë§¥ë½: 19.0

ğŸ‰ ê²€ì¦ í†µê³¼! (ì‹œë„ 1íšŒ)

ë‹µë³€>
ì¡°ì„  ë¯¼ì¡±ì´ ì§„ì •ìœ¼ë¡œ ì¼ë³¸ ì œêµ­ì˜ ì¼ì›ì´ ë˜ê³ ì í•œë‹¤ë©´...
[ì´ê´‘ìˆ˜ ìŠ¤íƒ€ì¼ì˜ ë‹µë³€]

[ê²€ì¦ ì ìˆ˜: 78.5/100]
[ì¶œì²˜: ì°½ì”¨ê°œëª….pdf, ì´ê´‘ìˆ˜ì˜ ì¹œì¼ì´ë… ë‹¤ì‹œ ì½ê¸°.pdf]
```

### ì˜ˆì œ 2: Python ì½”ë“œë¡œ ì‚¬ìš©

```python
from agents import MultiAgentOrchestrator

# ì´ˆê¸°í™”
orchestrator = MultiAgentOrchestrator(
    talk_style_dir="./GS_talk_style",
    paper_dir="./GS_paper",
    max_retries=3
)

# ì§ˆë¬¸ ì²˜ë¦¬
result = orchestrator.process_query(
    "ì´ê´‘ìˆ˜ì˜ ë¯¼ì¡± ê°œì¡°ë¡ ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    verbose=True
)

# ê²°ê³¼ í™•ì¸
print(result['final_answer'])
print(f"ì ìˆ˜: {result['validation_score']}/100")
print(f"ì¶œì²˜: {result['knowledge_sources']}")
```

### ì˜ˆì œ 3: ê°œë³„ ì—ì´ì „íŠ¸ ì‚¬ìš©

```python
from agents import KnowledgeAgent, StyleAgent, ValidatorAgent

# 1. ì§€ì‹ ê²€ìƒ‰
knowledge = KnowledgeAgent()
kb_result = knowledge.process({
    "query": "ì´ê´‘ìˆ˜ì˜ ì§•ë³‘ ê´€ë ¨ ê¸€",
    "top_k": 5
})

print(kb_result['answer'])

# 2. ìŠ¤íƒ€ì¼ ë³€í™˜
style = StyleAgent()
styled_result = style.process({
    "text": kb_result['answer'],
    "context": "ì§•ë³‘ ê´€ë ¨"
})

print(styled_result['styled_text'])

# 3. ê²€ì¦
validator = ValidatorAgent(style_agent=style)
val_result = validator.process({
    "generated_text": styled_result['styled_text'],
    "original_query": "ì§•ë³‘ ê´€ë ¨"
})

print(f"ê²€ì¦ ì ìˆ˜: {val_result['score']}/100")
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### âŒ "Connection refused" ì˜¤ë¥˜

**ì›ì¸**: Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```bash
# ì„œë²„ ì‹¤í–‰
ollama serve
```

### âŒ "model not found" ì˜¤ë¥˜

**ì›ì¸**: ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1
ollama pull nomic-embed-text

# ì„¤ì¹˜ í™•ì¸
ollama list
```

### âŒ "No module named 'langchain_community'"

**ì›ì¸**: íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```bash
pip install -r requirements.txt
```

### âš ï¸ ë„ˆë¬´ ëŠë ¤ìš”

**ì›ì¸**: GPUê°€ ì—†ê±°ë‚˜ í° ëª¨ë¸ ì‚¬ìš©

**í•´ê²°**:

1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©:
```bash
ollama pull mistral  # 4.1GB, ë” ë¹ ë¦„
```

2. ì½”ë“œì—ì„œ ëª¨ë¸ ë³€ê²½:
```python
orchestrator = MultiAgentOrchestrator(
    # ... 
)
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ OLLAMA_MODEL=mistral ì„¤ì •
```

3. GPU ì‚¬ìš© í™•ì¸:
```bash
nvidia-smi  # NVIDIA GPU í™•ì¸
```

### ğŸ“Š ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì›ì¸**: RAMì´ ë¶€ì¡±í•¨

**í•´ê²°**:

1. ë” ì‘ì€ ëª¨ë¸:
```bash
ollama pull phi3  # 2.3GB, ê°€ë²¼ì›€
```

2. chunk_size ì¤„ì´ê¸° (ì½”ë“œ ìˆ˜ì • í•„ìš”)

## ğŸ’¡ íŒ

### í•œêµ­ì–´ ì„±ëŠ¥ í–¥ìƒ

```bash
# Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í•œêµ­ì–´ ìš°ìˆ˜)
ollama pull qwen2.5:7b
```

`.env` íŒŒì¼:
```
OLLAMA_MODEL=qwen2.5:7b
```

### í’ˆì§ˆ í–¥ìƒ

```bash
# ë” í° ëª¨ë¸ ì‚¬ìš© (GPU ê¶Œì¥)
ollama pull llama3.1:70b  # 40GB, ë§¤ìš° ë†’ì€ í’ˆì§ˆ
```

### ì†ë„ í–¥ìƒ

```bash
# ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
ollama pull mistral
```

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. âœ… **ê¸°ë³¸ ì‹¤í–‰ ì™„ë£Œ** â†’ `python main.py`
2. ğŸ“– **ì˜ˆì œ ì‹¤í–‰** â†’ `python examples.py`
3. ğŸ”§ **ì½”ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•** â†’ ëª¨ë¸/íŒŒë¼ë¯¸í„° ë³€ê²½
4. ğŸ“Š **ì„±ëŠ¥ íŠœë‹** â†’ [OLLAMA_SETUP.md](OLLAMA_SETUP.md) ì°¸ê³ 

## â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q: ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•œê°€ìš”?
A: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì—ë§Œ í•„ìš”. ì´í›„ ì™„ì „ ì˜¤í”„ë¼ì¸ ê°€ëŠ¥!

### Q: OpenAI API í‚¤ê°€ í•„ìš”í•œê°€ìš”?
A: ì•„ë‹ˆìš”! OllamaëŠ” ì™„ì „ ë¬´ë£Œ & ë¡œì»¬ì…ë‹ˆë‹¤.

### Q: GPUê°€ í•„ìˆ˜ì¸ê°€ìš”?
A: ì•„ë‹ˆìš”. CPUë§Œìœ¼ë¡œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼. GPU ê¶Œì¥.

### Q: ì–´ë–¤ GPUê°€ í•„ìš”í•œê°€ìš”?
A: NVIDIA GPU (CUDA ì§€ì›). ìµœì†Œ 8GB VRAM ê¶Œì¥.

### Q: ë¹„ìš©ì´ ë“œë‚˜ìš”?
A: ì™„ì „ ë¬´ë£Œ! ì „ê¸°ì„¸ë§Œ ë‚˜ê°‘ë‹ˆë‹¤ ğŸ˜„

## ğŸ‰ ì„±ê³µ!

ì´ì œ ë¬´ë£Œë¡œ ë¡œì»¬ì—ì„œ ì´ê´‘ìˆ˜ ì¹œì¼ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ë” ìì„¸í•œ ë‚´ìš©ì€:
- [README.md](README.md) - ì „ì²´ í”„ë¡œì íŠ¸ ì„¤ëª…
- [OLLAMA_SETUP.md](OLLAMA_SETUP.md) - ìƒì„¸ Ollama ê°€ì´ë“œ
