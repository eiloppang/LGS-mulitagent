#!/bin/bash

# ì´ê´‘ìˆ˜ ì¹œì¼ ì±—ë´‡ - ê°€ìƒí™˜ê²½ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (A100 GPU ìµœì í™”)

set -e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨

echo "=========================================="
echo "ì´ê´‘ìˆ˜ ì¹œì¼ ì±—ë´‡ ì„¤ì¹˜ ì‹œìž‘"
echo "GPU: NVIDIA A100 40GB"
echo "=========================================="
echo ""

# í˜„ìž¬ ë””ë ‰í† ë¦¬ í™•ì¸
cd /home/work/gayeon_mulitagent

# 1. ê°€ìƒí™˜ê²½ ìƒì„±
echo "ðŸ”¨ [1/5] ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
if [ -d "envs" ]; then
    echo "   âš ï¸  ê¸°ì¡´ envs í´ë”ê°€ ìžˆìŠµë‹ˆë‹¤. ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“¤ê¹Œìš”? (y/n)"
    read -r response
    if [ "$response" = "y" ]; then
        rm -rf envs
        python -m venv envs
        echo "   âœ“ ê°€ìƒí™˜ê²½ ìž¬ìƒì„± ì™„ë£Œ"
    else
        echo "   â­ï¸  ê¸°ì¡´ ê°€ìƒí™˜ê²½ ì‚¬ìš©"
    fi
else
    python -m venv envs
    echo "   âœ“ ê°€ìƒí™˜ê²½ ìƒì„± ì™„ë£Œ"
fi

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source envs/bin/activate
echo "   âœ“ ê°€ìƒí™˜ê²½ í™œì„±í™”: $(which python)"
echo ""

# 2. pip ì—…ê·¸ë ˆì´ë“œ
echo "ðŸ“¦ [2/5] pip ì—…ê·¸ë ˆì´ë“œ ì¤‘..."
pip install --upgrade pip -q
echo "   âœ“ pip ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ"
echo ""

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "ðŸ“¥ [3/5] íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
pip install -r requirements.txt
echo "   âœ“ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ"
echo ""

# 4. Ollama í™•ì¸
echo "ðŸ” [4/5] Ollama ìƒíƒœ í™•ì¸ ì¤‘..."
if command -v ollama &> /dev/null; then
    echo "   âœ“ Ollama ì„¤ì¹˜ í™•ì¸ë¨"
    
    # Ollama ì‹¤í–‰ í™•ì¸
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "   âœ“ Ollama ì„œë²„ ì‹¤í–‰ ì¤‘"
    else
        echo "   âš ï¸  Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"
        echo "   ðŸ“Œ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ 'ollama serve' ì‹¤í–‰ í•„ìš”"
    fi
else
    echo "   âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"
    echo "   ðŸ“Œ ì„¤ì¹˜ ë°©ë²•: curl -fsSL https://ollama.com/install.sh | sh"
fi
echo ""

# 5. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì œì•ˆ
echo "ðŸ¤– [5/5] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´"
echo ""
echo "A100 40GB GPUë¥¼ ìœ„í•œ ì¶”ì²œ ëª¨ë¸:"
echo ""
echo "ì¶”ì²œ 1) qwen2.5:32b (í•œêµ­ì–´ ìµœê°•, 18GB)"
echo "  ollama pull qwen2.5:32b"
echo ""
echo "ì¶”ì²œ 2) llama3.1:70b (ìµœê³  í’ˆì§ˆ, 40GB)"
echo "  ollama pull llama3.1:70b"
echo ""
echo "í•„ìˆ˜) ìž„ë² ë”© ëª¨ë¸ (274MB)"
echo "  ollama pull nomic-embed-text"
echo ""
echo "ì§€ê¸ˆ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (1/2/n)"
echo "1: qwen2.5:32b (ì¶”ì²œ)"
echo "2: llama3.1:70b"
echo "n: ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ"
read -r choice

if [ "$choice" = "1" ]; then
    echo "ðŸ“¥ qwen2.5:32b ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 18GB, ì‹œê°„ ì†Œìš”)"
    ollama pull qwen2.5:32b
    ollama pull nomic-embed-text
    MODEL="qwen2.5:32b"
    echo "   âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
elif [ "$choice" = "2" ]; then
    echo "ðŸ“¥ llama3.1:70b ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 40GB, ì‹œê°„ ì†Œìš”)"
    ollama pull llama3.1:70b
    ollama pull nomic-embed-text
    MODEL="llama3.1:70b"
    echo "   âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ"
else
    MODEL="llama3.1"
    echo "   â­ï¸  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€"
fi
echo ""

# 6. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±
echo "âš™ï¸  í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì¤‘..."
cat > .env << EOF
# Ollama ì„¤ì •
OLLAMA_BASE_URL=http://localhost:11434

# ì‚¬ìš©í•  ëª¨ë¸ (A100 40GB ìµœì í™”)
OLLAMA_MODEL=$MODEL

# ìž„ë² ë”© ëª¨ë¸
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
EOF
echo "   âœ“ .env íŒŒì¼ ìƒì„± ì™„ë£Œ"
echo ""

# ì™„ë£Œ ë©”ì‹œì§€
echo "=========================================="
echo "âœ… ì„¤ì¹˜ ì™„ë£Œ!"
echo "=========================================="
echo ""
echo "ðŸ“‹ ë‹¤ìŒ ë‹¨ê³„:"
echo ""
echo "1. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸:"
echo "   ìƒˆ í„°ë¯¸ë„ì—ì„œ: ollama serve"
echo ""
echo "2. ê°€ìƒí™˜ê²½ í™œì„±í™” (ìƒˆ í„°ë¯¸ë„ì„ ì—´ ë•Œë§ˆë‹¤):"
echo "   source envs/bin/activate"
echo ""
echo "3. í”„ë¡œê·¸ëž¨ ì‹¤í–‰:"
echo "   python main.py"
echo ""
echo "4. ì˜ˆì œ ì‹¤í–‰:"
echo "   python examples.py"
echo ""
echo "=========================================="
echo "ðŸ“š ë¬¸ì„œ ì°¸ê³ :"
echo "  - GPU_SETUP.md: A100 ìµœì í™” ê°€ì´ë“œ"
echo "  - QUICKSTART.md: ë¹ ë¥¸ ì‹œìž‘"
echo "  - OLLAMA_SETUP.md: Ollama ìƒì„¸ ê°€ì´ë“œ"
echo "=========================================="
